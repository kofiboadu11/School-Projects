{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b12814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-table:\n",
      "State 0: tensor([-0.1961, -0.1085, -0.0865, -0.1836], grad_fn=<ViewBackward0>)\n",
      "State 1: tensor([-0.1954, -0.1096, -0.0861, -0.1907], grad_fn=<ViewBackward0>)\n",
      "State 2: tensor([-0.1944, -0.1089, -0.0864, -0.1904], grad_fn=<ViewBackward0>)\n",
      "State 3: tensor([-0.1944, -0.1071, -0.0818, -0.1885], grad_fn=<ViewBackward0>)\n",
      "State 4: tensor([-0.2006, -0.1133, -0.0909, -0.1893], grad_fn=<ViewBackward0>)\n",
      "State 5: tensor([-0.1990, -0.1114, -0.0892, -0.1855], grad_fn=<ViewBackward0>)\n",
      "State 6: tensor([-0.2031, -0.1138, -0.0934, -0.1813], grad_fn=<ViewBackward0>)\n",
      "State 7: tensor([-0.1959, -0.1104, -0.0884, -0.1910], grad_fn=<ViewBackward0>)\n",
      "State 8: tensor([-0.2005, -0.1120, -0.0884, -0.1872], grad_fn=<ViewBackward0>)\n",
      "State 9: tensor([-0.1990, -0.1124, -0.0902, -0.1902], grad_fn=<ViewBackward0>)\n",
      "State 10: tensor([-0.1992, -0.1106, -0.0850, -0.1867], grad_fn=<ViewBackward0>)\n",
      "State 11: tensor([-0.1943, -0.1101, -0.0882, -0.1931], grad_fn=<ViewBackward0>)\n",
      "State 12: tensor([-0.1965, -0.1106, -0.0883, -0.1899], grad_fn=<ViewBackward0>)\n",
      "State 13: tensor([-0.1983, -0.1127, -0.0939, -0.1876], grad_fn=<ViewBackward0>)\n",
      "State 14: tensor([-0.1990, -0.1136, -0.0960, -0.1871], grad_fn=<ViewBackward0>)\n",
      "State 15: tensor([-0.1982, -0.1105, -0.0878, -0.1860], grad_fn=<ViewBackward0>)\n",
      "State 16: tensor([-0.1951, -0.1079, -0.0842, -0.1871], grad_fn=<ViewBackward0>)\n",
      "State 17: tensor([-0.1928, -0.1072, -0.0824, -0.1918], grad_fn=<ViewBackward0>)\n",
      "State 18: tensor([-0.1920, -0.1042, -0.0770, -0.1867], grad_fn=<ViewBackward0>)\n",
      "State 19: tensor([-0.1971, -0.1112, -0.0873, -0.1926], grad_fn=<ViewBackward0>)\n",
      "State 20: tensor([-0.1965, -0.1101, -0.0891, -0.1872], grad_fn=<ViewBackward0>)\n",
      "State 21: tensor([-0.1987, -0.1100, -0.0856, -0.1853], grad_fn=<ViewBackward0>)\n",
      "State 22: tensor([-0.1966, -0.1092, -0.0845, -0.1869], grad_fn=<ViewBackward0>)\n",
      "State 23: tensor([-0.1951, -0.1083, -0.0822, -0.1903], grad_fn=<ViewBackward0>)\n",
      "State 24: tensor([-0.1973, -0.1089, -0.0813, -0.1888], grad_fn=<ViewBackward0>)\n",
      "State 25: tensor([-0.1985, -0.1120, -0.0893, -0.1903], grad_fn=<ViewBackward0>)\n",
      "State 26: tensor([-0.2011, -0.1138, -0.0920, -0.1885], grad_fn=<ViewBackward0>)\n",
      "State 27: tensor([-0.1976, -0.1115, -0.0912, -0.1871], grad_fn=<ViewBackward0>)\n",
      "State 28: tensor([-0.1979, -0.1117, -0.0906, -0.1880], grad_fn=<ViewBackward0>)\n",
      "State 29: tensor([-0.1953, -0.1104, -0.0887, -0.1916], grad_fn=<ViewBackward0>)\n",
      "State 30: tensor([-0.1977, -0.1120, -0.0916, -0.1889], grad_fn=<ViewBackward0>)\n",
      "State 31: tensor([-0.1952, -0.1090, -0.0854, -0.1890], grad_fn=<ViewBackward0>)\n",
      "State 32: tensor([-0.1987, -0.1120, -0.0920, -0.1858], grad_fn=<ViewBackward0>)\n",
      "State 33: tensor([-0.1974, -0.1102, -0.0863, -0.1883], grad_fn=<ViewBackward0>)\n",
      "State 34: tensor([-0.1972, -0.1094, -0.0867, -0.1841], grad_fn=<ViewBackward0>)\n",
      "State 35: tensor([-0.1983, -0.1121, -0.0911, -0.1885], grad_fn=<ViewBackward0>)\n",
      "State 36: tensor([-0.1990, -0.1108, -0.0848, -0.1896], grad_fn=<ViewBackward0>)\n",
      "State 37: tensor([-0.1966, -0.1120, -0.0911, -0.1929], grad_fn=<ViewBackward0>)\n",
      "State 38: tensor([-0.1922, -0.1065, -0.0777, -0.1950], grad_fn=<ViewBackward0>)\n",
      "State 39: tensor([-0.1968, -0.1100, -0.0855, -0.1895], grad_fn=<ViewBackward0>)\n",
      "State 40: tensor([-0.1948, -0.1085, -0.0851, -0.1884], grad_fn=<ViewBackward0>)\n",
      "State 41: tensor([-0.1966, -0.1096, -0.0849, -0.1891], grad_fn=<ViewBackward0>)\n",
      "State 42: tensor([-0.1968, -0.1106, -0.0882, -0.1886], grad_fn=<ViewBackward0>)\n",
      "State 43: tensor([-0.1921, -0.1080, -0.0840, -0.1950], grad_fn=<ViewBackward0>)\n",
      "State 44: tensor([-0.1950, -0.1093, -0.0871, -0.1890], grad_fn=<ViewBackward0>)\n",
      "State 45: tensor([-0.1968, -0.1115, -0.0891, -0.1929], grad_fn=<ViewBackward0>)\n",
      "State 46: tensor([-0.1957, -0.1092, -0.0854, -0.1896], grad_fn=<ViewBackward0>)\n",
      "State 47: tensor([-0.1974, -0.1110, -0.0870, -0.1914], grad_fn=<ViewBackward0>)\n",
      "State 48: tensor([-0.1950, -0.1087, -0.0835, -0.1916], grad_fn=<ViewBackward0>)\n",
      "State 49: tensor([-0.1988, -0.1114, -0.0903, -0.1848], grad_fn=<ViewBackward0>)\n",
      "State 50: tensor([-0.2012, -0.1128, -0.0888, -0.1878], grad_fn=<ViewBackward0>)\n",
      "State 51: tensor([-0.1950, -0.1073, -0.0795, -0.1902], grad_fn=<ViewBackward0>)\n",
      "State 52: tensor([-0.1965, -0.1093, -0.0868, -0.1862], grad_fn=<ViewBackward0>)\n",
      "State 53: tensor([-0.1981, -0.1117, -0.0893, -0.1899], grad_fn=<ViewBackward0>)\n",
      "State 54: tensor([-0.1971, -0.1099, -0.0866, -0.1875], grad_fn=<ViewBackward0>)\n",
      "State 55: tensor([-0.1926, -0.1064, -0.0782, -0.1931], grad_fn=<ViewBackward0>)\n",
      "State 56: tensor([-0.1979, -0.1120, -0.0905, -0.1908], grad_fn=<ViewBackward0>)\n",
      "State 57: tensor([-0.2007, -0.1135, -0.0923, -0.1874], grad_fn=<ViewBackward0>)\n",
      "State 58: tensor([-0.1967, -0.1100, -0.0869, -0.1875], grad_fn=<ViewBackward0>)\n",
      "State 59: tensor([-0.1973, -0.1092, -0.0840, -0.1871], grad_fn=<ViewBackward0>)\n",
      "Episode 1, Total Reward: -87465\n",
      "Episode 2, Total Reward: -103145\n",
      "Final Q-table:\n",
      "State 0: tensor([-2.7150, -2.2521, -2.5360, -5.2109], grad_fn=<ViewBackward0>)\n",
      "State 1: tensor([-2.6838, -2.0119, -2.4894, -4.4990], grad_fn=<ViewBackward0>)\n",
      "State 2: tensor([-2.6931, -2.0835, -2.5025, -4.7125], grad_fn=<ViewBackward0>)\n",
      "State 3: tensor([-2.8039, -2.9361, -2.6722, -7.2323], grad_fn=<ViewBackward0>)\n",
      "State 4: tensor([-2.8194, -3.0551, -2.6959, -7.5839], grad_fn=<ViewBackward0>)\n",
      "State 5: tensor([-2.8272, -3.1151, -2.7078, -7.7612], grad_fn=<ViewBackward0>)\n",
      "State 6: tensor([-2.8464, -3.2626, -2.7372, -8.1971], grad_fn=<ViewBackward0>)\n",
      "State 7: tensor([ -3.1464,  -5.7299,  -3.0909, -15.4207], grad_fn=<ViewBackward0>)\n",
      "State 8: tensor([ -3.2213,  -6.7971,  -3.1571, -18.4894], grad_fn=<ViewBackward0>)\n",
      "State 9: tensor([ -3.3585,  -8.3919,  -3.3024, -23.1039], grad_fn=<ViewBackward0>)\n",
      "State 10: tensor([-2.7409, -2.4509, -2.5756, -5.7985], grad_fn=<ViewBackward0>)\n",
      "State 11: tensor([-2.7055, -2.1787, -2.5214, -4.9940], grad_fn=<ViewBackward0>)\n",
      "State 12: tensor([-2.7519, -2.5357, -2.5925, -6.0490], grad_fn=<ViewBackward0>)\n",
      "State 13: tensor([-2.7030, -2.1597, -2.5176, -4.9378], grad_fn=<ViewBackward0>)\n",
      "State 14: tensor([ -3.0014,  -4.4710,  -2.9637, -11.7613], grad_fn=<ViewBackward0>)\n",
      "State 15: tensor([-2.8437, -3.2418, -2.7330, -8.1358], grad_fn=<ViewBackward0>)\n",
      "State 16: tensor([-2.8899, -3.5967, -2.8037, -9.1846], grad_fn=<ViewBackward0>)\n",
      "State 17: tensor([ -3.0766,  -5.1175,  -3.0335, -13.6429], grad_fn=<ViewBackward0>)\n",
      "State 18: tensor([ -2.9511,  -4.0792,  -2.8902, -10.6056], grad_fn=<ViewBackward0>)\n",
      "State 19: tensor([ -3.2509,  -7.2515,  -3.2140, -19.8083], grad_fn=<ViewBackward0>)\n",
      "State 20: tensor([-2.7339, -2.3975, -2.5650, -5.6406], grad_fn=<ViewBackward0>)\n",
      "State 21: tensor([-2.6984, -2.1244, -2.5106, -4.8335], grad_fn=<ViewBackward0>)\n",
      "State 22: tensor([-2.7097, -2.2111, -2.5279, -5.0896], grad_fn=<ViewBackward0>)\n",
      "State 23: tensor([-2.7587, -2.5881, -2.6029, -6.2037], grad_fn=<ViewBackward0>)\n",
      "State 24: tensor([-2.7450, -2.4829, -2.5820, -5.8928], grad_fn=<ViewBackward0>)\n",
      "State 25: tensor([ -2.9638,  -4.1654,  -2.9169, -10.8651], grad_fn=<ViewBackward0>)\n",
      "State 26: tensor([ -2.9559,  -4.1048,  -2.9048, -10.6862], grad_fn=<ViewBackward0>)\n",
      "State 27: tensor([ -3.0480,  -4.8436,  -3.0258, -12.8565], grad_fn=<ViewBackward0>)\n",
      "State 28: tensor([ -3.2344,  -6.7157,  -3.2075, -18.2862], grad_fn=<ViewBackward0>)\n",
      "State 29: tensor([ -3.6669, -14.2828,  -3.6051, -39.9834], grad_fn=<ViewBackward0>)\n",
      "State 30: tensor([-2.7777, -2.7343, -2.6320, -6.6359], grad_fn=<ViewBackward0>)\n",
      "State 31: tensor([-2.7205, -2.2941, -2.5444, -5.3350], grad_fn=<ViewBackward0>)\n",
      "State 32: tensor([-2.8402, -3.2151, -2.7277, -8.0568], grad_fn=<ViewBackward0>)\n",
      "State 33: tensor([-2.9231, -3.8523, -2.8545, -9.9398], grad_fn=<ViewBackward0>)\n",
      "State 34: tensor([ -2.9762,  -4.2605,  -2.9358, -11.1462], grad_fn=<ViewBackward0>)\n",
      "State 35: tensor([ -3.0944,  -5.2316,  -3.0762, -13.9899], grad_fn=<ViewBackward0>)\n",
      "State 36: tensor([ -3.2544,  -6.6569,  -3.1925, -18.1191], grad_fn=<ViewBackward0>)\n",
      "State 37: tensor([ -3.1237,  -5.5067,  -3.0881, -14.7817], grad_fn=<ViewBackward0>)\n",
      "State 38: tensor([ -3.3852,  -9.3478,  -3.3310, -25.8267], grad_fn=<ViewBackward0>)\n",
      "State 39: tensor([ -4.1990, -23.3419,  -4.1308, -65.9852], grad_fn=<ViewBackward0>)\n",
      "State 40: tensor([-2.8558, -3.3350, -2.7516, -8.4112], grad_fn=<ViewBackward0>)\n",
      "State 41: tensor([  -7.0448,  -72.9252,   -7.0057, -208.2809], grad_fn=<ViewBackward0>)\n",
      "State 42: tensor([  -6.8842,  -71.1841,   -6.8257, -203.2302], grad_fn=<ViewBackward0>)\n",
      "State 43: tensor([  -5.7618,  -50.9683,   -5.7122, -145.2522], grad_fn=<ViewBackward0>)\n",
      "State 44: tensor([  -6.3436,  -60.8581,   -6.2879, -173.6398], grad_fn=<ViewBackward0>)\n",
      "State 45: tensor([  -6.8603,  -70.4295,   -6.8062, -201.0814], grad_fn=<ViewBackward0>)\n",
      "State 46: tensor([  -6.1729,  -58.6115,   -6.0951, -167.1516], grad_fn=<ViewBackward0>)\n",
      "State 47: tensor([ -4.6851, -31.9754,  -4.6340, -90.7603], grad_fn=<ViewBackward0>)\n",
      "State 48: tensor([  -5.2281,  -41.9648,   -5.1610, -119.3954], grad_fn=<ViewBackward0>)\n",
      "State 49: tensor([  -5.3798,  -45.1903,   -5.2950, -128.6181], grad_fn=<ViewBackward0>)\n",
      "State 50: tensor([ -11.9827, -157.0954,  -11.9782, -449.9092], grad_fn=<ViewBackward0>)\n",
      "State 51: tensor([  -5.4394,  -44.8707,   -5.3548, -127.7599], grad_fn=<ViewBackward0>)\n",
      "State 52: tensor([  -5.4033,  -45.4205,   -5.3220, -129.2881], grad_fn=<ViewBackward0>)\n",
      "State 53: tensor([  -5.5113,  -46.5340,   -5.4631, -132.5313], grad_fn=<ViewBackward0>)\n",
      "State 54: tensor([  -5.5736,  -47.9213,   -5.5113, -136.4930], grad_fn=<ViewBackward0>)\n",
      "State 55: tensor([  -5.5516,  -46.9292,   -5.4729, -133.6651], grad_fn=<ViewBackward0>)\n",
      "State 56: tensor([  -5.2209,  -41.7116,   -5.1607, -118.6775], grad_fn=<ViewBackward0>)\n",
      "State 57: tensor([  -5.3359,  -43.5671,   -5.2801, -124.0105], grad_fn=<ViewBackward0>)\n",
      "State 58: tensor([  -5.5536,  -47.4951,   -5.4864, -135.2710], grad_fn=<ViewBackward0>)\n",
      "State 59: tensor([  -5.3169,  -42.5693,   -5.2623, -121.1765], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 166\u001b[0m\n\u001b[0;32m    162\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(q_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    164\u001b[0m episode_rewards\u001b[38;5;241m=\u001b[39mq_learning(env, q_network, optimizer, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(episode_rewards)\n\u001b[0;32m    167\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    168\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Reward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2813\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2814\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:465\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# Don't allow any None value; these would be up-converted to one\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# element array of None which causes problems downstream.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m tup):\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx, y, and format string must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m kw \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop_name, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinestyle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    469\u001b[0m                           (linestyle, marker, color)):\n",
      "\u001b[1;31mValueError\u001b[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = (rows - 1, 0)  # Bottom left corner\n",
    "        self.goal = (rows - 1, cols - 1)  # Bottom right corner\n",
    "        self.cliff = [(rows - 1, j) for j in range(1, cols - 1)]  # Cliff cells\n",
    "        self.agent_pos = self.start\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        next_row, next_col = self.agent_pos\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            next_row -= 1\n",
    "        elif action == 1:  # Down\n",
    "            next_row += 1\n",
    "        elif action == 2:  # Left\n",
    "            next_col -= 1\n",
    "        elif action == 3:  # Right\n",
    "            next_col += 1\n",
    "\n",
    "        next_row = max(0, min(next_row, self.rows - 1))\n",
    "        next_col = max(0, min(next_col, self.cols - 1))\n",
    "\n",
    "        self.agent_pos = (next_row, next_col)\n",
    "\n",
    "        reward = -5  # Default reward for moving\n",
    "\n",
    "        if self.agent_pos in self.cliff:\n",
    "            reward = -500  # Cliff penalty\n",
    "            self.agent_pos = self.start  # Reset to start if fallen into the cliff\n",
    "\n",
    "        done = self.agent_pos == self.goal\n",
    "\n",
    "        return self.agent_pos, reward, done, {}\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.fc3 = nn.Linear(8, 16)\n",
    "        self.fc4 = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def epsilon_greedy_action(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(len(q_values))\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "def q_learning(env, q_network, optimizer, num_episodes, epsilon_decay):\n",
    "    gamma = 0.7\n",
    "    epsilon = 0.9\n",
    "    input_dim = env.rows * env.cols\n",
    "    output_dim = 4  # Four possible actions: Up, Down, Left, Right\n",
    "\n",
    "    print(\"Initial Q-table:\")\n",
    "    print_q_table(q_network, input_dim)\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_one_hot = np.zeros(input_dim)\n",
    "            state_one_hot[state[0] * env.cols + state[1]] = 1\n",
    "\n",
    "            q_values = q_network(torch.tensor(state_one_hot, dtype=torch.float32))\n",
    "            action = epsilon_greedy_action(q_values.data.numpy(), epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                q_values_target = reward\n",
    "            else:\n",
    "                next_state_one_hot = np.zeros(input_dim)\n",
    "                next_state_one_hot[next_state[0] * env.cols + next_state[1]] = 1\n",
    "                next_q_values = q_network(torch.tensor(next_state_one_hot, dtype=torch.float32))\n",
    "                q_values_target = reward + gamma * torch.max(next_q_values).item()\n",
    "\n",
    "            q_values[action] = q_values_target\n",
    "\n",
    "            loss = nn.MSELoss()(q_network(torch.tensor(state_one_hot, dtype=torch.float32)), q_values.unsqueeze(0))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        epsilon *= epsilon_decay\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    print(\"Final Q-table:\")\n",
    "    print_q_table(q_network, input_dim)\n",
    "\n",
    "def print_q_table(q_network, input_dim):\n",
    "    for state in range(input_dim):\n",
    "        state_one_hot = np.zeros(input_dim)\n",
    "        state_one_hot[state] = 1\n",
    "        q_values = q_network(torch.tensor(state_one_hot, dtype=torch.float32))\n",
    "        print(f\"State {state}: {q_values}\")\n",
    "        \n",
    "        \n",
    "def visualize_episode(env, q_network):\n",
    "    cmap = ListedColormap(['white', 'red', 'green', 'blue'])\n",
    "    grid = np.zeros((env.rows, env.cols))\n",
    "    grid[env.start] = 2  # Start position\n",
    "    grid[env.goal] = 3  # Goal position\n",
    "    for cliff_pos in env.cliff:\n",
    "        grid[cliff_pos] = 1  # Cliff positions\n",
    "\n",
    "    state = env.reset()\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    while True:\n",
    "        plt.imshow(grid, cmap=cmap, origin='upper')\n",
    "        plt.title(\"Grid World\")\n",
    "        plt.pause(0.5)\n",
    "        plt.cla()\n",
    "\n",
    "        state_one_hot = np.zeros(env.rows * env.cols)\n",
    "        state_one_hot[state[0] * env.cols + state[1]] = 1\n",
    "        q_values = q_network(torch.tensor(state_one_hot, dtype=torch.float32)).detach().numpy()\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    plt.imshow(grid, cmap=cmap, origin='upper')\n",
    "    plt.title(\"Grid World\")\n",
    "    plt.pause(0.5)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld(6, 10)\n",
    "\n",
    "    input_dim = env.rows * env.cols\n",
    "    output_dim = 4  # Four possible actions: Up, Down, Left, Right\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "    episode_rewards=q_learning(env, q_network, optimizer, num_episodes=2, epsilon_decay=0.99)\n",
    "    \n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward per Episode')\n",
    "    plt.show()\n",
    "\n",
    "    visualize_episode(env, q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba616b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2133bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
